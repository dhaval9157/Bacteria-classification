{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7361974,"sourceType":"datasetVersion","datasetId":4276460}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import img_to_array, array_to_img, load_img\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, fbeta_score, precision_score, recall_score\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential, Model\nfrom keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Dense, Dropout, Flatten, BatchNormalization\nfrom keras.optimizers import Adam\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import img_to_array, array_to_img, load_img\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, fbeta_score, precision_score, recall_score\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport glob\n# import wandb\n# from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-21T11:40:02.178251Z","iopub.execute_input":"2024-02-21T11:40:02.179001Z","iopub.status.idle":"2024-02-21T11:40:17.397938Z","shell.execute_reply.started":"2024-02-21T11:40:02.178973Z","shell.execute_reply":"2024-02-21T11:40:17.396939Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-21 11:40:04.405006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-21 11:40:04.405099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-21 11:40:04.563907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"image_path = \"/kaggle/input/bacteria-figshare/\"\n# tmp_data_path = \"/content/tmp_data/\"\nroot_dir = '/kaggle/working/data/'","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:40:17.400014Z","iopub.execute_input":"2024-02-21T11:40:17.400712Z","iopub.status.idle":"2024-02-21T11:40:17.407077Z","shell.execute_reply.started":"2024-02-21T11:40:17.400677Z","shell.execute_reply":"2024-02-21T11:40:17.405989Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# INPUT_SIZE = 71\n# input_shape = (INPUT_SIZE, INPUT_SIZE, 3)  # Assuming color images (3 channels)\n# num_classes = 24\n# batch_size = 32\n\n# def pad_array(input_array, target_shape=(224, 224, 3)):\n#     # Get the current shape of the input array\n#     current_shape = input_array.shape\n\n#     # Calculate the difference between the target shape and current shape\n#     pad_width = [(0, max(0, target_shape[i] - current_shape[i])) for i in range(len(target_shape))]\n\n#     # Pad the array using numpy.pad\n#     padded_array = np.pad(input_array, pad_width, mode='constant', constant_values=0)\n\n#     return padded_array\n\n# import cv2\n# import os\n# import glob\n# import matplotlib.pyplot as plt\n# import tensorflow as tf\n# from keras.preprocessing.image import ImageDataGenerator, array_to_img\n# import numpy as np\n# from skimage.transform import resize\n# from PIL import Image\n\n# def generate_images_from_and_to_given_path(image_path, output_path, num_augmented_images, MAX_SIZE):\n#     INPUT_SIZE = MAX_SIZE\n   \n#     datagen = ImageDataGenerator(\n#     rotation_range=30,\n#     width_shift_range=0.05,\n#     height_shift_range=0.05,\n#     shear_range=0.2,\n#     zoom_range=0.3,\n#     horizontal_flip=True,\n#     brightness_range=[0.5, 1.5]\n#     )\n   \n#     for annotated_file in glob.glob(os.path.join(f'{image_path}annot_YOLO/', '*.txt')):\n#         with open(annotated_file) as f:        \n#             tmp_list = []\n#             for text in f:\n#                 second_tmp = []\n#                 for i, coordinates in enumerate(text.split(' ')):\n#                     if i == 4:\n#                         second_tmp.append(coordinates.strip())\n#                     else:\n#                         second_tmp.append(coordinates)\n#                 tmp_list.append(second_tmp)\n\n#             # For image cropping\n#             image = cv2.imread(f'{image_path}{annotated_file[-14:-4]}.jpg')\n#             boxes = tmp_list\n\n#             shift = 0.01\n#             for i, box in enumerate(boxes):\n#                 x_min = float(box[1])\n#                 y_min = float(box[2])\n#                 x_min = float(box[1])\n#                 y_min = float(box[2])\n#                 width = float(box[3]) + 0.014\n#                 height = float(box[4])+ 0.014\n\n#                 x_max = x_min + width\n#                 y_max = y_min + height\n#                 cropped = image[int(y_min*image.shape[0]):int(y_max*image.shape[0]),\n#                               int(x_min*image.shape[1]):int(x_max*image.shape[1])]\n\n#                 if cropped.size == 0:\n#                     print(\"Locha: \", annotated_file)\n#                 else:\n#                     if cropped.shape[0] > MAX_SIZE or cropped.shape[1] > MAX_SIZE:\n#                         cropped = cv2.resize(cropped, (MAX_SIZE, MAX_SIZE))\n\n#                     # Generate and plot augmented images\n#                     cropped_batch = np.expand_dims(cropped, axis=0)  # Add batch dimension\n#                     generate_and_save_image_to_given_path(output_path+\"\", cropped_batch, annotated_file, datagen, num_augmented_images, INPUT_SIZE)\n\n# def generate_and_save_image_to_given_path(output_path, cropped_batch, annotated_file, datagen, num_augmented_images, INPUT_SIZE):\n#     count =0\n#     # for getting colonies coordinates\n#     if not os.path.exists(f'{output_path}{annotated_file[-14:-10]}'):\n#         os.makedirs(f'{output_path}{annotated_file[-14:-10]}')\n   \n#     for i, augmented_img in enumerate(datagen.flow(cropped_batch, batch_size=1)):\n#         augmented_img = pad_array(augmented_img[0], target_shape=(INPUT_SIZE, INPUT_SIZE, 3))\n\n# #         augmented_img = cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB)\n#         augmented_img = np.array(augmented_img).astype(np.uint8)\n       \n# #         min_val,max_val=augmented_img.min(),augmented_img.max()\n# #         augmented_img = 255.0*(augmented_img - min_val)/(max_val - min_val)\n# #         augmented_img = augmented_img.astype(np.uint8)\n       \n#         image = Image.fromarray(augmented_img, 'RGB')\n#         image.save(f'{output_path}{annotated_file[-14:-10]}/{count}_{annotated_file[-14:-4]}.jpg', bbox_inches='tight', pad_inches=0)\n        \n# #         plt.imshow(cv2.cvtColor(augmented_img, cv2.COLOR_BGR2RGB))\n# #         plt.axis('off')  # Turn off axis\n# #         plt.savefig(f'{output_path}{annotated_file[-14:-10]}/{count}_{annotated_file[-14:-4]}.jpg', bbox_inches='tight', pad_inches=0)  # Save the image\n# #         plt.show()\n       \n#         count+=1\n\n#         if i == num_augmented_images - 1:\n#             break\n           \n           \n# generate_images_from_and_to_given_path(\"/kaggle/input/bacteria-figshare/\", \"/kaggle/working/augmented_dataset2/\", num_augmented_images=15, MAX_SIZE=71)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:40:17.408816Z","iopub.execute_input":"2024-02-21T11:40:17.409212Z","iopub.status.idle":"2024-02-21T11:40:17.440313Z","shell.execute_reply.started":"2024-02-21T11:40:17.409170Z","shell.execute_reply":"2024-02-21T11:40:17.439502Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"INPUT_SIZE = 75\ninput_shape = (INPUT_SIZE, INPUT_SIZE, 3)  # Assuming color images (3 channels)\nnum_classes = 24\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:40:17.442858Z","iopub.execute_input":"2024-02-21T11:40:17.443201Z","iopub.status.idle":"2024-02-21T11:40:17.454776Z","shell.execute_reply.started":"2024-02-21T11:40:17.443173Z","shell.execute_reply":"2024-02-21T11:40:17.453984Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def pad_array(input_array, target_shape=(224, 224, 3)):\n    # Get the current shape of the input array\n    current_shape = input_array.shape\n\n    # Calculate the difference between the target shape and current shape\n    pad_width = [(0, max(0, target_shape[i] - current_shape[i])) for i in range(len(target_shape))]\n\n    # Pad the array using numpy.pad\n    padded_array = np.pad(input_array, pad_width, mode='constant', constant_values=0)\n\n    return padded_array","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:40:17.455715Z","iopub.execute_input":"2024-02-21T11:40:17.455992Z","iopub.status.idle":"2024-02-21T11:40:17.470371Z","shell.execute_reply.started":"2024-02-21T11:40:17.455971Z","shell.execute_reply":"2024-02-21T11:40:17.469592Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for annotated_file in glob.glob(os.path.join('/kaggle/input/bacteria-figshare/annot_YOLO/', '*.txt')):\n    with open(annotated_file) as f:\n        # for getting colonies coordinates\n        if os.path.exists(f'/kaggle/working/data/{annotated_file[-14:-10]}') == False:\n            os.makedirs(f'/kaggle/working/data/{annotated_file[-14:-10]}')\n        tmp_list = []\n        for text in f:\n            second_tmp = []\n            for i, coordinates in enumerate(text.split(' ')):\n                if i == 4:\n                    second_tmp.append(coordinates.strip())\n                else:\n                    second_tmp.append(coordinates)\n            tmp_list.append(second_tmp)\n\n        # For image croping\n        image = cv2.imread(f'{image_path}{annotated_file[-14:-4]}.jpg')\n        boxes = tmp_list\n\n        shift = 0.01\n\n        for i, box in enumerate(boxes):\n            x_min = float(box[1]) - shift\n            y_min = float(box[2]) - shift\n            width = float(box[3])\n            height = float(box[4])\n\n            x_max = x_min + width\n            y_max = y_min + height\n\n            cropped = image[int(y_min*image.shape[0]):int(y_max*image.shape[0]),\n                          int(x_min*image.shape[1]):int(x_max*image.shape[1])]\n\n            cropped = pad_array(cropped, target_shape=(INPUT_SIZE, INPUT_SIZE, 3))\n\n            save_path = f'/kaggle/working/data/{annotated_file[-14:-10]}/'+str(i)+f'_{annotated_file[-14:-4]}.jpg'\n            if(cropped.size==0):\n                print(\"Locha: \", annotated_file)\n            else:\n                cv2.imwrite(save_path, cropped)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:40:17.471336Z","iopub.execute_input":"2024-02-21T11:40:17.471608Z","iopub.status.idle":"2024-02-21T11:41:24.702948Z","shell.execute_reply.started":"2024-02-21T11:40:17.471587Z","shell.execute_reply":"2024-02-21T11:41:24.701917Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# import joblib\n# model = joblib.load('/kaggle/input/densnet/keras/ntohing/1/XceptionNet.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:41:24.704434Z","iopub.execute_input":"2024-02-21T11:41:24.704766Z","iopub.status.idle":"2024-02-21T11:41:24.708687Z","shell.execute_reply.started":"2024-02-21T11:41:24.704741Z","shell.execute_reply":"2024-02-21T11:41:24.707832Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"root_dir = '/kaggle/working/data'\ndatagen = ImageDataGenerator(\n#     rescale=1./255,\n#     rotation_range=10,\n#     width_shift_range=0.1,\n#     height_shift_range=0.1,\n#     vertical_flip=True,\n#     fill_mode='nearest',\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    root_dir,\n    target_size=(INPUT_SIZE, INPUT_SIZE),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nval_generator = datagen.flow_from_directory(\n    root_dir,\n    target_size=(INPUT_SIZE, INPUT_SIZE),\n    batch_size=batch_size,\n    shuffle=False,\n    class_mode='categorical',\n    subset='validation'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:41:24.709731Z","iopub.execute_input":"2024-02-21T11:41:24.710031Z","iopub.status.idle":"2024-02-21T11:41:27.281812Z","shell.execute_reply.started":"2024-02-21T11:41:24.710006Z","shell.execute_reply":"2024-02-21T11:41:27.280964Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Found 45502 images belonging to 24 classes.\nFound 11363 images belonging to 24 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n# from tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Nadam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.metrics import Precision, Recall\n\nbatch_size = 32\n# Create Xception model\nbase_model = keras.applications.EfficientNetB3(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# for layer in base_model.layers:\n#     layer.trainable = False\n\n    \nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='sigmoid'))\nmodel.add(Dense(1024, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1024, activation='sigmoid'))\nmodel.add(Dropout(0.5))  # Dropout layer for regularization\nmodel.add(Dense(num_classes, activation='softmax'))\n\n\n\n# # Compile the model\nmodel.compile(optimizer=Nadam(weight_decay=0.00001), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n\n# Data Augmentation\n\n# Reduce learning rate when a metric has stopped improving\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:41:27.283058Z","iopub.execute_input":"2024-02-21T11:41:27.283418Z","iopub.status.idle":"2024-02-21T11:41:33.301650Z","shell.execute_reply.started":"2024-02-21T11:41:27.283385Z","shell.execute_reply":"2024-02-21T11:41:33.300666Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n31790344/31790344 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Train the model\nepochs = 20\nmodel.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // batch_size,\n    epochs=epochs,\n    validation_data=val_generator,\n    validation_steps=val_generator.samples // batch_size,\n    callbacks = [reduce_lr]\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T11:41:33.304478Z","iopub.execute_input":"2024-02-21T11:41:33.304766Z","iopub.status.idle":"2024-02-21T12:38:52.121263Z","shell.execute_reply.started":"2024-02-21T11:41:33.304742Z","shell.execute_reply":"2024-02-21T12:38:52.120263Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2024-02-21 11:42:15.617407: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/efficientnetb2/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708515745.560833     114 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"1421/1421 [==============================] - 281s 123ms/step - loss: 0.7602 - accuracy: 0.7786 - precision: 0.8598 - recall: 0.7228 - val_loss: 0.4045 - val_accuracy: 0.8835 - val_precision: 0.8973 - val_recall: 0.8727 - lr: 0.0010\nEpoch 2/20\n1421/1421 [==============================] - 169s 119ms/step - loss: 0.3285 - accuracy: 0.9012 - precision: 0.9185 - recall: 0.8858 - val_loss: 0.2328 - val_accuracy: 0.9329 - val_precision: 0.9423 - val_recall: 0.9268 - lr: 0.0010\nEpoch 3/20\n1421/1421 [==============================] - 169s 119ms/step - loss: 0.2603 - accuracy: 0.9214 - precision: 0.9345 - recall: 0.9113 - val_loss: 0.2448 - val_accuracy: 0.9243 - val_precision: 0.9329 - val_recall: 0.9198 - lr: 0.0010\nEpoch 4/20\n1421/1421 [==============================] - 168s 118ms/step - loss: 0.2257 - accuracy: 0.9310 - precision: 0.9430 - recall: 0.9220 - val_loss: 0.2400 - val_accuracy: 0.9279 - val_precision: 0.9394 - val_recall: 0.9210 - lr: 0.0010\nEpoch 5/20\n1421/1421 [==============================] - 167s 117ms/step - loss: 0.1973 - accuracy: 0.9419 - precision: 0.9508 - recall: 0.9341 - val_loss: 0.2063 - val_accuracy: 0.9342 - val_precision: 0.9441 - val_recall: 0.9246 - lr: 0.0010\nEpoch 6/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.1859 - accuracy: 0.9455 - precision: 0.9545 - recall: 0.9395 - val_loss: 0.2019 - val_accuracy: 0.9408 - val_precision: 0.9498 - val_recall: 0.9343 - lr: 0.0010\nEpoch 7/20\n1421/1421 [==============================] - 163s 115ms/step - loss: 0.1723 - accuracy: 0.9514 - precision: 0.9584 - recall: 0.9453 - val_loss: 0.1950 - val_accuracy: 0.9400 - val_precision: 0.9498 - val_recall: 0.9335 - lr: 0.0010\nEpoch 8/20\n1421/1421 [==============================] - 169s 119ms/step - loss: 0.1578 - accuracy: 0.9547 - precision: 0.9616 - recall: 0.9489 - val_loss: 0.1881 - val_accuracy: 0.9455 - val_precision: 0.9525 - val_recall: 0.9403 - lr: 0.0010\nEpoch 9/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.1474 - accuracy: 0.9572 - precision: 0.9635 - recall: 0.9525 - val_loss: 0.1713 - val_accuracy: 0.9528 - val_precision: 0.9603 - val_recall: 0.9471 - lr: 0.0010\nEpoch 10/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.1373 - accuracy: 0.9601 - precision: 0.9657 - recall: 0.9559 - val_loss: 0.2485 - val_accuracy: 0.9321 - val_precision: 0.9401 - val_recall: 0.9281 - lr: 0.0010\nEpoch 11/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.1406 - accuracy: 0.9600 - precision: 0.9660 - recall: 0.9558 - val_loss: 0.1842 - val_accuracy: 0.9506 - val_precision: 0.9567 - val_recall: 0.9470 - lr: 0.0010\nEpoch 12/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.1294 - accuracy: 0.9635 - precision: 0.9686 - recall: 0.9597 - val_loss: 0.2101 - val_accuracy: 0.9358 - val_precision: 0.9431 - val_recall: 0.9315 - lr: 0.0010\nEpoch 13/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.0609 - accuracy: 0.9819 - precision: 0.9842 - recall: 0.9801 - val_loss: 0.1162 - val_accuracy: 0.9675 - val_precision: 0.9713 - val_recall: 0.9664 - lr: 2.0000e-04\nEpoch 14/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.0411 - accuracy: 0.9878 - precision: 0.9890 - recall: 0.9868 - val_loss: 0.1125 - val_accuracy: 0.9703 - val_precision: 0.9730 - val_recall: 0.9690 - lr: 2.0000e-04\nEpoch 15/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.0357 - accuracy: 0.9897 - precision: 0.9907 - recall: 0.9888 - val_loss: 0.1194 - val_accuracy: 0.9697 - val_precision: 0.9723 - val_recall: 0.9687 - lr: 2.0000e-04\nEpoch 16/20\n1421/1421 [==============================] - 165s 116ms/step - loss: 0.0289 - accuracy: 0.9911 - precision: 0.9916 - recall: 0.9906 - val_loss: 0.1165 - val_accuracy: 0.9720 - val_precision: 0.9744 - val_recall: 0.9710 - lr: 2.0000e-04\nEpoch 17/20\n1421/1421 [==============================] - 164s 116ms/step - loss: 0.0275 - accuracy: 0.9918 - precision: 0.9927 - recall: 0.9911 - val_loss: 0.1317 - val_accuracy: 0.9699 - val_precision: 0.9722 - val_recall: 0.9683 - lr: 2.0000e-04\nEpoch 18/20\n1421/1421 [==============================] - 166s 117ms/step - loss: 0.0198 - accuracy: 0.9945 - precision: 0.9950 - recall: 0.9941 - val_loss: 0.1225 - val_accuracy: 0.9732 - val_precision: 0.9750 - val_recall: 0.9725 - lr: 4.0000e-05\nEpoch 19/20\n1421/1421 [==============================] - 169s 119ms/step - loss: 0.0153 - accuracy: 0.9953 - precision: 0.9957 - recall: 0.9949 - val_loss: 0.1235 - val_accuracy: 0.9745 - val_precision: 0.9760 - val_recall: 0.9737 - lr: 4.0000e-05\nEpoch 20/20\n1421/1421 [==============================] - 170s 119ms/step - loss: 0.0133 - accuracy: 0.9957 - precision: 0.9961 - recall: 0.9956 - val_loss: 0.1292 - val_accuracy: 0.9734 - val_precision: 0.9749 - val_recall: 0.9727 - lr: 4.0000e-05\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x799aa057b4f0>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, top_k_accuracy_score, confusion_matrix, precision_score\nimport numpy as np\n\n# Assuming you already have the trained model and validation data generator (val_generator) from the previous code\n\n# Evaluate the model on the validation set\npredictions = model.predict(val_generator)\n\n# Convert predictions to class labels\npredicted_labels = np.argmax(predictions, axis=1)\n\n# Convert true labels to class labels\ntrue_labels = val_generator.classes\n\n# Calculate top-3 and top-4 accuracy\ntop3_accuracy = top_k_accuracy_score(true_labels, predictions, k=3)\ntop5_accuracy = top_k_accuracy_score(true_labels, predictions, k=5)\n\n# Calculate overall accuracy\noverall_accuracy = accuracy_score(true_labels, predicted_labels)\n\n# Calculate recall, F1 score, and classification report\nrecall = recall_score(true_labels, predicted_labels, average='weighted')\nprecision = precision_score(true_labels, predicted_labels, average='weighted')\nf1 = f1_score(true_labels, predicted_labels, average='weighted')\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\nclassification_rep = classification_report(true_labels, predicted_labels)\n\n# Print the metrics\nprint(f'Top-3 Accuracy: {top3_accuracy * 100:.2f}%')\nprint(f'Top-5 Accuracy: {top5_accuracy * 100:.2f}%')\nprint(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\nprint(f'Recall: {recall * 100:.2f}%')\nprint(f'Precision: {precision * 100:.2f}%')\nprint(f'F1 Score: {f1 * 100:.2f}%')\nprint(conf_matrix)\nprint('Classification Report:')\nprint(classification_rep)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:38:56.814765Z","iopub.execute_input":"2024-02-21T12:38:56.815497Z","iopub.status.idle":"2024-02-21T12:39:07.226607Z","shell.execute_reply.started":"2024-02-21T12:38:56.815466Z","shell.execute_reply":"2024-02-21T12:39:07.225534Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"356/356 [==============================] - 10s 22ms/step\nTop-3 Accuracy: 99.59%\nTop-5 Accuracy: 99.78%\nOverall Accuracy: 97.34%\nRecall: 97.34%\nPrecision: 97.37%\nF1 Score: 97.34%\n[[  75    0    0    0    1    0    0    0    0    0    0    0    0    0\n     0    0    0    1    0    0    1    1    0    0]\n [   0  304    0    0    1    0    0    0    0    0    0    0    0    0\n     0    1    0    0    0    0    0    0    0    0]\n [   1    1   53    0    3    0    0    0    0    0    0    0    0    0\n     0    1    0    0    0    0    0    0    0    0]\n [   0    0    0   59    1    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0]\n [   0    0    0    1  816    0    0    0    0    0    0    0    0    0\n     1    0    0    1    0    0    1    0    0    0]\n [   0    0    0    0    0 1049    1    0    0    8    0    0    0    1\n     0    0    0    0    9    0    0    2   31    1]\n [   0    0    0    0    2    0  204    0    7    1    0    0    0    0\n     0    1    0    1    0    0    0    0    1    0]\n [   0    0    0    0    0    1    0   70    0    0    0    0    0    0\n     0    0    0    2    0    0    0    0    0    0]\n [   0    0    0    0    1    1    3    0  345    2    0    0    0    0\n     0    0    0    0    0    0    0    0    3    0]\n [   0    0    0    0    0    1    1    0    0  848    0    0    0    0\n     0    0    0    0    0    0    0    0   22    0]\n [   0    0    0    0    0    0    0    0    0    0   78    0    0    0\n     0    0    0    0    0   17    0    0    1    0]\n [   0    0    0    0    1    1    0    0    1    0    0   87    0    0\n     0    1    0    1    0    0    0    0    0    0]\n [   0    0    0    0    0    0    0    0    0    0    0    0  359    0\n     0    0    0    0    0    0    0    0    0    0]\n [   0    0    0    0    0    1    0    0    0    0    0    0    0  218\n     0    0    0    0    0    0    0    1    0    0]\n [   0    0    0    0    2    0    0    0    0    0    0    0    0    0\n   164    1    0    4    0    0    2    0    0    0]\n [   0    1    0    0    0    0    0    0    1    1    0    0    0    0\n     0  265    0    0    1    0    0    0    0    0]\n [   0    0    0    0    0    0    0    4    0    0    0    0    0    0\n     0    0   40    0    0    0    0    0    0    0]\n [   1    0    0    0    0    0    1    0    0    0    0    0    0    0\n     0    0    0  274    0    0    0    0    0    0]\n [   0    0    0    0    0    2    0    0    0    0    0    0    0    0\n     0    2    0    2  544    0    1    2    3    0]\n [   0    0    0    0    0    0    0    0    0    0    3    0    0    0\n     0    0    0    0    0  167    0    0    0    0]\n [   0    0    1    0    3    1    0    0    3    1    0    0    1    0\n     2    0    0    0    1    0 2217    0    2    0]\n [   5    0    2    0    0    2    0    0    0    1    0    0    1    1\n     2    0    0    0    1    0    1 1339    8    0]\n [   0    0    0    0    2   15    2    0    3   38    0    1    1    0\n     0    1    0    0    0    0    0    4 1344    2]\n [   0    1    2    0    0    0    0    0    0    0    0    0    0    0\n     0    2    0    1    1    0    0    0    8  142]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.95      0.93        79\n           1       0.99      0.99      0.99       306\n           2       0.91      0.90      0.91        59\n           3       0.98      0.98      0.98        60\n           4       0.98      1.00      0.99       820\n           5       0.98      0.95      0.96      1102\n           6       0.96      0.94      0.95       217\n           7       0.95      0.96      0.95        73\n           8       0.96      0.97      0.97       355\n           9       0.94      0.97      0.96       872\n          10       0.96      0.81      0.88        96\n          11       0.99      0.95      0.97        92\n          12       0.99      1.00      1.00       359\n          13       0.99      0.99      0.99       220\n          14       0.97      0.95      0.96       173\n          15       0.96      0.99      0.97       269\n          16       1.00      0.91      0.95        44\n          17       0.95      0.99      0.97       276\n          18       0.98      0.98      0.98       556\n          19       0.91      0.98      0.94       170\n          20       1.00      0.99      1.00      2232\n          21       0.99      0.98      0.99      1363\n          22       0.94      0.95      0.95      1413\n          23       0.98      0.90      0.94       157\n\n    accuracy                           0.97     11363\n   macro avg       0.97      0.96      0.96     11363\nweighted avg       0.97      0.97      0.97     11363\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('EfficientNetB1.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-02-21T12:40:00.816512Z","iopub.execute_input":"2024-02-21T12:40:00.817129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndef plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    Plot the confusion matrix for multi-class classification.\n\n    Parameters:\n        y_true (array-like): True labels.\n        y_pred (array-like): Predicted labels.\n        classes (list): List of class names.\n        title (str): Title of the plot.\n        cmap (matplotlib.colors.Colormap): Colormap for the plot.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n\n    plt.figure(figsize=(len(classes)+2, len(classes)+2))\n    sns.set(font_scale=1.2)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, linewidths=.5, square=True,\n                xticklabels=classes, yticklabels=classes)\n\n    plt.title(title)\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.show()\n\n# Example usage:\n# Replace y_true and y_pred with your actual and predicted labels\n\nplot_confusion_matrix(true_labels, predicted_labels, classes=[\"sp01\", \"sp02\", \"sp03\", \"sp04\", \"sp05\", \"sp06\", \"sp07\", \"sp08\", \"sp09\", \"sp10\", \"sp11\", \"sp12\", \"sp13\", \"sp14\", \"sp15\", \"sp16\", \"sp17\", \"sp18\", \"sp19\", \"sp20\", \"sp21\", \"sp22\", \"sp23\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\n\n# Preprocess the image\nimg_path = '/kaggle/working/data/sp21/299_sp21_img51.jpg'\nimg = image.load_img(img_path, target_size=(75,75,3))\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\n\nmodel.predict(img_array).argmax()+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}